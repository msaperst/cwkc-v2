# Simple workflow for updating our results file
name: Update results from multiple locations

on:
  # Runs every five minutes to grab updated data
  schedule:
    - cron: '*/5 * * * *'

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# Allow only one concurrent data check, skipping runs queued between the run in-progress and latest queued.
# However, do NOT cancel in-progress runs as we want to allow these updates to complete.
concurrency:
  group: "updates"
  cancel-in-progress: false

jobs:
  # Single deploy job since we're just updating
  update:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup credentials
        run: |
          cd scraper
          echo ${{ secrets.GSPREADCREDENTIALS }} > spreadsheet_credentials.json

      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.12 #install the python needed

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f scraper/requirements.txt ]; then pip install -r scraper/requirements.txt; fi

      - name: Update results
        run: |
          cd scraper
          python getData.py

      - name: Publish new results
        uses: test-room-7/action-update-file@v2
        with:
          file-path: public/assets/csv/results.csv
          commit-msg: Updating results
          github-token: ${{ secrets.GITHUB_TOKEN }}
